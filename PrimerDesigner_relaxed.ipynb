{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OrensteinLab/PrimerDesigner/blob/main/Tracking_Timings_Creation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-cQNE8To4yW"
      },
      "source": [
        "#Pre-Calculations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFTaJFH_qB2p"
      },
      "source": [
        "##Installs & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5Qvu79FpBUL",
        "outputId": "f1c61766-543b-420c-f28d-b4de029cc147"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.10/dist-packages (3.3.4)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.0)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.10/dist-packages (from bokeh) (3.1.3)\n",
            "Requirement already satisfied: contourpy>=1 in /usr/local/lib/python3.10/dist-packages (from bokeh) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.10/dist-packages (from bokeh) (1.25.2)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.10/dist-packages (from bokeh) (23.2)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from bokeh) (9.4.0)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.10/dist-packages (from bokeh) (6.0.1)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.10/dist-packages (from bokeh) (6.3.2)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.10/dist-packages (from bokeh) (2023.10.1)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.9->bokeh) (2.1.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.48.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Collecting seequence\n",
            "  Cloning https://github.com/FordyceLab/seequence.git to /tmp/pip-install-gum8rkv0/seequence_7b33976eaff540238436abf8da4ba80d\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/FordyceLab/seequence.git /tmp/pip-install-gum8rkv0/seequence_7b33976eaff540238436abf8da4ba80d\n",
            "  Resolved https://github.com/FordyceLab/seequence.git to commit 3ea730537fcf5b7ef807ebf6e057f5bf4e875bb9\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from seequence) (1.25.2)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.10/dist-packages (from seequence) (3.3.4)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.10/dist-packages (from bokeh->seequence) (3.1.3)\n",
            "Requirement already satisfied: contourpy>=1 in /usr/local/lib/python3.10/dist-packages (from bokeh->seequence) (1.2.0)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.10/dist-packages (from bokeh->seequence) (23.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from bokeh->seequence) (2.2.0)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from bokeh->seequence) (9.4.0)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.10/dist-packages (from bokeh->seequence) (6.0.1)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.10/dist-packages (from bokeh->seequence) (6.3.2)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.10/dist-packages (from bokeh->seequence) (2023.10.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.9->bokeh->seequence) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->bokeh->seequence) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->bokeh->seequence) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->bokeh->seequence) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2->bokeh->seequence) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U bokeh seaborn pandas\n",
        "!pip install primer3-py biopython pandarallel\n",
        "!pip install pulp\n",
        "!pip install gurobipy\n",
        "!pip install biopython==1.75\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nu59O-XqpDCW"
      },
      "outputs": [],
      "source": [
        "# IMPORTS\n",
        "import time\n",
        "\n",
        "import random as rand\n",
        "from itertools import product\n",
        "\n",
        "from pandarallel import pandarallel as pl\n",
        "pl.initialize()\n",
        "\n",
        "import primer3 as p3\n",
        "from Bio.Seq import Seq\n",
        "\n",
        "import itertools as it\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('display.precision', 1)\n",
        "import networkx as nx\n",
        "\n",
        "import gurobipy as gp\n",
        "\n",
        "import tracemalloc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def revcomp(seq):\n",
        "  return str(Seq(seq).reverse_complement())\n",
        "def translate(seq):\n",
        "  return str(Seq(seq).translate())\n",
        "\n",
        "print('Ready')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXfAgj6lqJQn"
      },
      "source": [
        "\n",
        "\n",
        "## Create Primer sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlY4e_TaqpUb"
      },
      "outputs": [],
      "source": [
        "def subsequences(sequence, lmin, lmax): #Generates all subsequences w/ all poss. start-stop pairs\n",
        "  ls = []\n",
        "  for j in range(lmin, lmax+1): #length\n",
        "    for i in range(len(sequence)-j+1): #starting index\n",
        "      start = i\n",
        "      stop = i+j\n",
        "      ls.append([sequence[start:stop], start, stop, stop-start])\n",
        "  return pd.DataFrame(ls, columns=['seq','start','stop','len'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VZtd11nmgBD"
      },
      "source": [
        "## Primer Data\n",
        "##### Creates the primer pandas dataframe containing information about every possible primer in the protein coding sequence. The function calculates various primer parameters including gc content, delta G and melting temepratures. The cost is also calculated for every primer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxLGrHMfqu60"
      },
      "outputs": [],
      "source": [
        "def create_primer_df():\n",
        "  # convention: start index of r-primers will be 3' (i.e. start < stop)\n",
        "  primer_f = pd.DataFrame(columns=['seq','start','stop','fr','len'])\n",
        "  primer_f[['seq','start','stop','len']] = subsequences(sequence_nt, primer_lmin, primer_lmax)\n",
        "  primer_f['fr'] = 'f'\n",
        "\n",
        "  #Shifting so that 0 is at the start of mutreg (upstream has negative values)\n",
        "  primer_f['start'] = primer_f.start - mutreg_start\n",
        "  primer_f['stop'] = primer_f.stop - mutreg_start\n",
        "\n",
        "  #Creating reverse primers at same locations\n",
        "  primer_r = primer_f[['seq','start','stop','fr','len']].copy()\n",
        "  primer_r['fr'] = 'r'\n",
        "  primer_r['seq'] = primer_r.seq.apply(revcomp)\n",
        "\n",
        "  #Concatenating Forward & Reverse\n",
        "  primer_df = pd.concat([primer_f,primer_r])\n",
        "  primer_df.sort_values(by=['start','stop','fr'], inplace=True)\n",
        "\n",
        "  #Calculating \"Cost\" Values\n",
        "  primer_df['gc'] = primer_df.seq.apply(GC)\n",
        "  primer_df['tm'] = primer_df.seq.apply(pcr.calcTm)\n",
        "  res = primer_df.seq.parallel_apply(lambda s: pcr.calcHairpin(s).todict())\n",
        "  primer_df['hp_tm'] = res.apply(lambda res: res['tm'])\n",
        "  primer_df['hp_dg'] = res.apply(lambda res: res['dg']*1e-3)\n",
        "  res = primer_df.seq.parallel_apply(lambda s: pcr.calcHomodimer(s).todict())\n",
        "  primer_df['ho_tm'] = res.apply(lambda res: res['tm'])\n",
        "  primer_df['ho_dg'] = res.apply(lambda res: res['dg']*1e-3)\n",
        "\n",
        "  def primer_cost(primer):\n",
        "    # calculates primer cost based on homodimer an haipin delta G, tm cost, and len cost\n",
        "    hp_dg_max = -4\n",
        "    ho_dg_max = -8\n",
        "    tm_min = 58\n",
        "\n",
        "    tm_cost = max(0, tm_min-primer.tm)**1.7\n",
        "    hp_cost = max(0, hp_dg_max - primer.hp_dg)**1.2\n",
        "    ho_cost = max(0, ho_dg_max - primer.ho_dg)**1.2\n",
        "    len_cost = primer.len*1e-5\n",
        "\n",
        "    # add high cost if there is a 4-mer of the nucleotide\n",
        "    sequence = primer.seq\n",
        "    for nucleotide in \"ATGC\":\n",
        "        if nucleotide * 4 in sequence:\n",
        "            cost += 1000000\n",
        "            break\n",
        "\n",
        "    cost = hp_cost + ho_cost + len_cost + tm_cost\n",
        "    return cost\n",
        "\n",
        "  primer_df['cost'] = primer_df.parallel_apply(primer_cost, axis=1)\n",
        "  primer_df['log10cost'] = primer_df.cost.apply(np.log10)\n",
        "\n",
        "  primer_df.reset_index(inplace=True)\n",
        "  primer_f = primer_df.query('fr==\"f\"').reset_index(drop=True)\n",
        "  primer_r = primer_df.query('fr==\"r\"').reset_index(drop=True)\n",
        "  primer_df.set_index(['start','stop','fr'], inplace=True)\n",
        "\n",
        "  return primer_f, primer_df\n",
        "\n",
        "def check_threshold(tm,gc):\n",
        "  # returns false only if the threshold flag is on and primers did not pass threshold\n",
        "  if not apply_threshold:\n",
        "    return True\n",
        "  else:\n",
        "    if min_gc <= gc <= max_gc and  min_tm <= tm <=max_tm:\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAbrEHcQnqXS"
      },
      "source": [
        "## Primer Class\n",
        "##### Creates a primer class which is used for the creation of the primer graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9Vws9ri2rd2"
      },
      "outputs": [],
      "source": [
        "class Primer:\n",
        "  def __init__(self,start,stop,is_r=False):\n",
        "    assert start < stop\n",
        "    self.start = start\n",
        "    self.stop = stop\n",
        "    self.is_r = is_r #forward or reverse\n",
        "    self.l = stop-start #length\n",
        "    self.w = primer_df.at[self.tup(),'cost'] #total cost value; the fancy notation is b/c\n",
        "                                             #of the hierarchal lookup system in panda.df\n",
        "\n",
        "  def __str__(self):\n",
        "    return ' '.join(map(str,(self.start, self.stop, self.is_r)))\n",
        "  def __repr__(self):\n",
        "    return f'{(\"r\" if self.is_r else\"f\")}({self.start},{self.stop})'\n",
        "  def tup(self):\n",
        "    return (self.start,self.stop,(\"r\" if self.is_r else\"f\"))\n",
        "\n",
        "\n",
        "def actions(primer): #returning possible counterparts (forward -> reverse; reverse -> forward)\n",
        "                     #i.e. this method gets the \"neighbors\"\n",
        "  if not primer.is_r:  # fwd\n",
        "    for oligo_l, primer_l in it.product(reversed(range(oligo_lmin, oligo_lmax+1)),\n",
        "                                        range(primer_lmin,primer_lmax+1)):\n",
        "\n",
        "      stop = primer.start + oligo_l\n",
        "      start = stop - primer_l\n",
        "\n",
        "      if apply_threshold:\n",
        "        # finds tm of forward and reverse primers\n",
        "        tm_f=primer_df.at[primer.tup(),'tm']\n",
        "        tm_r=primer_df.at[(start,stop,\"r\"),'tm']\n",
        "\n",
        "        # if tm difference is larger then max_difference threshold do not add primer to graph\n",
        "        if abs(tm_f-tm_r)>max_difference:\n",
        "          continue\n",
        "\n",
        "      yield Primer(start, stop, is_r=True)\n",
        "\n",
        "  elif primer.is_r:  # rev\n",
        "    for overlap_l, primer_l in it.product(reversed(range(overlap_lmin, overlap_lmax+1)),\n",
        "                                          range(primer_lmin,primer_lmax+1)):\n",
        "\n",
        "      start = primer.stop - overlap_l\n",
        "      stop = start + primer_l\n",
        "\n",
        "\n",
        "      # filter\n",
        "      no_split = (primer.start - stop) >= primer.start%3\n",
        "      if (stop > primer.start) or (not no_split): ## redundant to check first condition?\n",
        "        continue\n",
        "      yield Primer(start, stop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP2ck_c5nkiS"
      },
      "source": [
        "## Graph DFS\n",
        "##### Recursive function used for creating the primer graph representing all possible forward and reverse primer combinations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gal-RcyG2uBH"
      },
      "outputs": [],
      "source": [
        "def dfs(primer): #CREATING the graph\n",
        "\n",
        "  tm=primer_df.at[primer.tup(),'tm']\n",
        "  gc=primer_df.at[primer.tup(),'gc']\n",
        "\n",
        "  if (primer.start >= mutreg_l) and primer.is_r and check_threshold(tm,gc) :  # base case (end)\n",
        "    G.add_edge(primer.tup(),'d', weight=0.) #G is global variable defined in next section\n",
        "    return\n",
        "\n",
        "  for next_primer in actions(primer):\n",
        "\n",
        "    is_new = not G.has_node(next_primer.tup()) # check if primer node exists already\n",
        "\n",
        "    tm=primer_df.at[next_primer.tup(),'tm']\n",
        "    gc=primer_df.at[next_primer.tup(),'gc']\n",
        "\n",
        "    passes_threshold= check_threshold(tm,gc) # check if primer passes threshold\n",
        "\n",
        "    # only add edge if primer passes threshold\n",
        "    if passes_threshold:\n",
        "      G.add_edge(primer.tup(),next_primer.tup(), weight=next_primer.w) #weight is the cost of the new primer\n",
        "    if passes_threshold and is_new:\n",
        "      dfs(next_primer)\n",
        "\n",
        "def paths_ct(G, u, d): #total # of paths between two points\n",
        "    if u == d:\n",
        "        return 1\n",
        "    else:\n",
        "        if not G.nodes[u]: #npaths attribute is the # of paths out of u\n",
        "            G.nodes[u]['npaths'] = sum(paths_ct(G, c, d) for c in G.successors(u))\n",
        "        return G.nodes[u]['npaths']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DXHCtTQnVmR"
      },
      "source": [
        "## Greedy Algorithm\n",
        "##### Runs the baseline greedy algorithm which is compared to PrimerDesigner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceiNxaB-D1z1"
      },
      "outputs": [],
      "source": [
        "def run_greedy(G, primer_f, primer_df):\n",
        "  path_ls = [[]]\n",
        "  G_sub = G.copy()\n",
        "  for i in range(num_proteins):\n",
        "    nodes_to_remove = []\n",
        "    for p,n in it.product(path_ls[-1],G_sub.nodes()):\n",
        "      if n=='s' or n=='d':\n",
        "        continue\n",
        "      pn_intersect = n[1]-p[0] > allowed_overlap and p[1]-n[0] > allowed_overlap and n[2]==p[2]  ## check overlap\n",
        "      if pn_intersect:\n",
        "        nodes_to_remove.append(n)\n",
        "    G_sub.remove_nodes_from(set(nodes_to_remove))\n",
        "    try:\n",
        "      path_ls.append([primer for primer in nx.algorithms.shortest_path(G_sub,'s','d', weight='weight')][1:-1])\n",
        "    except:\n",
        "      print(f'WARNING: No feasible primer sequence for lib_{i}; reduce number of libraries or relax constraints.')\n",
        "\n",
        "  path_ls = path_ls[1:]\n",
        "\n",
        "  # # https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html\n",
        "  primer_set = pd.DataFrame()\n",
        "  for i,primer_ls in enumerate(path_ls):\n",
        "    primer_set1 = primer_df.loc[primer_ls].copy().reset_index()\n",
        "    primer_set1['lib_i'] = i\n",
        "    primer_set = pd.concat([primer_set, primer_set1])\n",
        "  primer_set['tile_i'] = primer_set.groupby(['lib_i','fr']).cumcount()\n",
        "  primer_set = primer_set[['lib_i','tile_i']+primer_set.columns[:-2].to_list()]\n",
        "\n",
        "  #print(primer_set.groupby('lib_i').cost.sum())\n",
        "  cost = primer_set.cost.sum()\n",
        "\n",
        "  return path_ls, cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ah8DwG1FeE6"
      },
      "source": [
        "##ILP Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZB76G7WmE9m"
      },
      "source": [
        "## Gurobi Setup\n",
        "##### The user should input their Gurobi license details for the ILP part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unOF64OiDddN"
      },
      "outputs": [],
      "source": [
        "def get_model():\n",
        "  params = {\n",
        "  \"WLSACCESSID\":\"\",\n",
        "  \"WLSSECRET\":\"\",\n",
        "  \"LICENSEID\":\n",
        "  }\n",
        "  env = gp.Env(params=params)\n",
        "\n",
        "  # Create the model within the Gurobi environment\n",
        "  model = gp.Model('min-sum', env=env)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AS8AHMSPoio-"
      },
      "source": [
        "## Model Creation and Run\n",
        "\n",
        "##### This function creates the ILP solver and represents the primer graph as an ILP optimization problem. It adds the ILP contraints based on overlap and single path contraints. Then, it runs the ILP solver and returns primer path solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjY5w7ZYFgV9"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def get_info(graphs):\n",
        "  setup_start = time.time()\n",
        "  tracemalloc.start()\n",
        "  #Create Model\n",
        "  model = get_model()\n",
        "  #Getting Nodes/Edges\n",
        "  graph_edges = G.edges(data=True)\n",
        "  graph_nodes = [node for node in G.nodes if node != 's' and node != 'd'] #removing s & d nodes\n",
        "\n",
        "  #for G in graphs:\n",
        "   # graphs_edges.append(G.edges(data=True))\n",
        "    #graphs_nodes.append(sorted((node for node in G.nodes if node != 's' and node != 'd'), key = lambda x:x[0:2])) #removing s & d\n",
        "\n",
        "  def create_bins(): # bins take the form (start, end, [])\n",
        "    all_bins = {(start, start+len, c):[] for start in range(*nt_range) for len in range(*l_range) for c in ('f', 'r')}\n",
        "\n",
        "    for node in graph_nodes: #for each node, add it into all the necessary bins\n",
        "      for bin_start in range(node[0], node[1]):\n",
        "        for bin_length in range(*l_range):\n",
        "          if bin_start + bin_length > node[1]:\n",
        "            break\n",
        "          else:\n",
        "            all_bins[(bin_start, bin_start + bin_length, node[2])].append(node)\n",
        "\n",
        "    # all_bins = {key:val for key,val in all_bins.items() if val} #no empty bins\n",
        "\n",
        "    return all_bins\n",
        "\n",
        "  all_bins = create_bins()\n",
        "  print(\"Number of Constraints:\", len(all_bins))\n",
        "  print(\"Average Vars Per Constraint\", 1/len(all_bins) * sum(len(val) for _, val in all_bins.items()))\n",
        "\n",
        "  def unite_bins():\n",
        "    # this function merges all bins corresponding to identical sequences\n",
        "    united_bins={}\n",
        "\n",
        "    for bin in all_bins.keys():\n",
        "      start=bin[0]\n",
        "      end=bin[1]\n",
        "      fr=bin[2]\n",
        "      length=end-start\n",
        "      bin_sequence = sequence_nt[start+mutreg_start:end+mutreg_start]\n",
        "      if (bin_sequence,fr) in united_bins:\n",
        "        continue\n",
        "      bin_union=[]\n",
        "      # find subsequences that are identical to the bin sequence and add them the the union of bins\n",
        "      for i in range(len(sequence_nt)-length+1):\n",
        "        if sequence_nt[i:i+length] == bin_sequence:\n",
        "          start = i - mutreg_start # subtract from start to account for upstream region\n",
        "          bin_union.extend(all_bins[(start, start + length,fr)]) # add the bins with identical sequences to the union list\n",
        "\n",
        "      # add bin union tp dictionary in the sequence entry\n",
        "      united_bins[(bin_sequence,fr)]=bin_union\n",
        "\n",
        "    return united_bins\n",
        "\n",
        "\n",
        "  # if the merge_bins flag is set to true, calls function to unite bins with identical sequences\n",
        "  if merge_bins:\n",
        "    united_bins= unite_bins()\n",
        "    print(\"Number of united bins constraints:\", len(united_bins))\n",
        "    print(\"Average Vars Per Constraint\", 1/len(united_bins) * sum(len(ls) for seq,ls in united_bins.items()))\n",
        "    all_bins= united_bins\n",
        "\n",
        "\n",
        "  #Converting Graphs to Lists\n",
        "  ij = gp.tuplelist()\n",
        "  w_ij = gp.tupledict()\n",
        "\n",
        "  for edge in graph_edges:\n",
        "    l = (str(edge[0]), str(edge[1])) #i, j\n",
        "    ij.append(l)\n",
        "    w_ij[l] = edge[-1]['weight']\n",
        "\n",
        "  print(\"Finished Conversion\")\n",
        "\n",
        "  #Adding Variables\n",
        "  x = model.addVars(ij, obj=w_ij, vtype=gp.GRB.BINARY)\n",
        "  print(\"Finished Variable Creations\")\n",
        "\n",
        "  # Intersection Constraints - bins\n",
        "  for cnt, nodes in enumerate(all_bins.values()):\n",
        "    all_edges = []\n",
        "    if cnt % (len(all_bins)//25) == 0:\n",
        "          print(int(cnt / len(all_bins) * 100))\n",
        "    for node in nodes:\n",
        "      all_edges.append(x.sum(str(node), '*'))\n",
        "    model.addConstr(gp.quicksum(all_edges) <= 1)\n",
        "  print(\"Finished Intersection constraints!\")\n",
        "\n",
        "  #Single Path Constraints\n",
        "  for n in graph_nodes + ['s', 'd']: #adding s & d back just here\n",
        "    v = str(n)\n",
        "    model.addConstr(sum(x[i,j] for i,j in ij.select(v, '*')) - sum(x[j,i] for j,i in ij.select('*', v)) == (num_proteins if v=='s' else -1 * num_proteins if v=='d' else 0), v)\n",
        "\n",
        "  setup_time = time.time() - setup_start\n",
        "  setup_memory = tracemalloc.get_traced_memory()[1] / 10**6\n",
        "  tracemalloc.stop()\n",
        "\n",
        "  tracemalloc.start()\n",
        "  start_time = time.time()\n",
        "  model.optimize()\n",
        "  ILP_time = time.time() - start_time\n",
        "  ILP_memory = tracemalloc.get_traced_memory()[1] / 10**6\n",
        "  tracemalloc.stop()\n",
        "\n",
        "  def post_processing(variables):\n",
        "    all_proteins = [['s'] for _ in range(num_proteins)]\n",
        "    true_edges = [index for index, var in x.items() if var.X != 0]\n",
        "\n",
        "    while true_edges:\n",
        "      edge = true_edges.pop(0)\n",
        "      added_edge = False\n",
        "      for protein_list in all_proteins:\n",
        "        if edge[0] == protein_list[-1]:\n",
        "          protein_list.append(edge[1])\n",
        "          added_edge = True\n",
        "          break\n",
        "      if not added_edge:\n",
        "        true_edges.append(edge)\n",
        "\n",
        "    return all_proteins\n",
        "\n",
        "  actual_values = post_processing(x)\n",
        "  for cnt, vals in enumerate(actual_values):\n",
        "    print(f\"Protein #{cnt+1} ({len(vals)})\")\n",
        "    print(vals)\n",
        "    print()\n",
        "\n",
        "  return model.numVars, model.numConstrs, setup_time, setup_memory, ILP_time, ILP_memory, actual_values, model.objVal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzIt-1aCqUAH"
      },
      "source": [
        "##Full Sequence\n",
        "##### The user can choose the mutreg sequence and upstream and downstream regions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfCGd8GAqMDQ"
      },
      "outputs": [],
      "source": [
        "# SpAP protein\n",
        "upstream_nt = 'GCTAGTGGTGCTAGCCCCGCGAAATTAATACGACTCACTATAGGGTCTAGAAATAATTTTGTTTAACTTTAAGAAGGAGATATACAT'\n",
        "mutreg_nt_full='ATGCAAAGCCCAGCACCTGCCGCAGCGCCTGCCCCTGCGGCACGTTCCATCGCAGCTACGCCTCCTAAACTGATCGTGGCAATTAGCGTGGACCAGTTTAGTGCAGACTTGTTCTCGGAGTATCGTCAATATTACACCGGAGGTTTAAAGCGTCTTACATCCGAAGGAGCTGTGTTCCCACGTGGTTATCAGAGTCATGCGGCAACAGAAACGTGTCCTGGTCACTCAACGATCCTGACAGGATCACGTCCGTCACGTACGGGTATTATCGCTAATAACTGGTTCGACTTGGACGCAAAGCGTGAGGATAAAAATCTGTACTGTGCTGAGGATGAATCCCAACCCGGTAGTTCGTCTGACAAGTACGAAGCTTCGCCACTGCACTTAAAGGTACCCACCCTGGGGGGACGCATGAAAGCCGCCAATCCTGCGACTCGTGTCGTCTCTGTTGCCGGCAAGGATCGCGCGGCCATTATGATGGGTGGCGCCACAGCGGATCAGGTCTGGTGGTTAGGGGGGCCTCAGGGGTATGTTTCGTATAAGGGTGTAGCGCCAACTCCCCTTGTAACACAGGTCAATCAGGCCTTTGCACAGCGCTTAGCTCAGCCGAACCCGGGATTTGAGTTGCCTGCTCAGTGCGTCAGCAAGGACTTTCCTGTTCAAGCGGGAAATCGCACAGTGGGTACCGGCCGCTTCGCCCGTGATGCTGGTGACTACAAAGGTTTTCGCATTTCCCCGGAGCAGGATGCTATGACGCTTGCATTCGCTGCCGCGGCCATTGAAAATATGCAATTAGGGAAGCAGGCCCAGACCGATATTATTAGCATTGGACTGAGCGCTACGGATTACGTGGGACACACCTTCGGCACGGAGGGTACGGAGAGTTGCATCCAAGTGGATCGTTTAGACACGGAGCTTGGTGCATTCTTTGATAAACTGGATAAGGATGGGATTGACTACGTAGTAGTGCTGACTGCAGATCATGGAGGACACGATCTGCCCGAACGTCATCGTATGAATGCCATGCCGATGGAACAGCGCGTAGACATGGCCCTGACACCTAAAGCTCTGAATGCTACCATCGCTGAGAAAGCTGGCCTTCCGGGCAAAAAGGTTATTTGGTCAGATGGACCTTCTGGCGATATTTACTATGATAAGGGCCTTACAGCCGCTCAACGTGCCCGTGTTGAAACCGAGGCGTTAAAATACTTGCGCGCGCATCCCCAAGTACAGACTGTATTCACTAAGGCGGAAATCGCGGCTACCCCTTCTCCGTCGGGACCACCTGAGAGCTGGAGTTTGATCCAGGAAGCTCGCGCGTCATTTTACCCGTCGCGCTCCGGGGACCTGTTACTTTTATTGAAACCTCGTGTGATGAGCATTCCTGAGCAAGCAGTCATGGGCTCGGTTGCAACCCATGGATCTCCATGGGATACGGATCGCCGTGTGCCTATCCTGTTTTGGCGCAAAGGTATGCAGCATTTCGAACAACCCTTAGGAGTAGAGACTGTTGATATTTTGCCCTCCTTGGCTGCACTTATTAAGCTTCCTGTTCCTAAGGATCAGATCGACGGCCGCTGTCTGGACTTGGTCGCCGGCAAGGATGATTCCTGTGCTGGACAGGGA'\n",
        "downstream_nt = 'GGAGGGTCTGGGGGAGGAGGCAGTGGCATGGTGAGCAAGGGCGAGGAGCTGTTCACCGGGGTGGTGCCCATCCTGGTCGAGCTGGACGGCGACGTAAACGGCCACAAGTTCAGCGTGTCCGGCGAGGGCGAGGGCGATGCCACCTACGGCAAGCTGACCCTGAAGTTCATCTGCACCACCGGCAAGCTGCCCG'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIKiSAUV6I7j"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1juYXyJqiRq"
      },
      "source": [
        "##Parameters\n",
        "##### The user can adjust algorithm parameters such as primer length, oligo length and allowed overlap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BthzfpIVklkP"
      },
      "outputs": [],
      "source": [
        "# adjust algorithm parameters\n",
        "primer_lmin, primer_lmax = 18, 30\n",
        "overlap_lmin,overlap_lmax = 45,50\n",
        "oligo_lmin,oligo_lmax = 195,205\n",
        "num_proteins = 3\n",
        "allowed_overlap = 6\n",
        "\n",
        "# by setting the unite bins to true, the user can choose to unite bins with identical sequences\n",
        "merge_bins=True\n",
        "\n",
        "# sets up pcr reactions with parameters\n",
        "pcr = p3.thermoanalysis.ThermoAnalysis(dna_conc= 250,\n",
        "                                       mv_conc= 50,\n",
        "                                       dv_conc= 0,\n",
        "                                       dntp_conc= 0,\n",
        "                                       tm_method= 'santalucia',\n",
        "                                       salt_correction_method= 'owczarzy',\n",
        "                                       temp_c= 25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0URNP9iplG5S"
      },
      "source": [
        "## Efficiency Thresholds\n",
        "##### The user can choose to apply primer efficiency threshold on gc content and tm and the maximum allowed tm different between forward and reverse primer in each pair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5Ia232clNyo"
      },
      "outputs": [],
      "source": [
        "\n",
        "apply_threshold= False # apply threshold flag. If true, threshold will be applied\n",
        "min_gc=40\n",
        "max_gc=60\n",
        "min_tm=58\n",
        "max_tm=65\n",
        "max_difference=3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDsUII2TqmbD"
      },
      "source": [
        "##Code To Run\n",
        "##### Runs PrimerDesigner on different sequence lengths. Records results in a dictionary which is written to a csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzokpLDXX59g"
      },
      "outputs": [],
      "source": [
        "all_data = []\n",
        "\n",
        "for seq_length in range(226,1626,100):\n",
        "\n",
        "  mutreg_nt=mutreg_nt_full[:seq_length]\n",
        "  sequence_nt = upstream_nt + mutreg_nt + downstream_nt\n",
        "  mutreg_l = len(mutreg_nt)\n",
        "  mutreg_start = len(upstream_nt)\n",
        "  mutreg_stop = mutreg_start + mutreg_l\n",
        "  mutreg_aa = translate(mutreg_nt)\n",
        "\n",
        "  nt_range = (-len(upstream_nt), len(mutreg_nt) + len(downstream_nt)+1) #range of nucleotides\n",
        "  l_range = (allowed_overlap+1, primer_lmax+1)\n",
        "\n",
        "  mut_df = create_mut_df()\n",
        "  primer_f, primer_df = create_primer_df()\n",
        "\n",
        "  #Creating the Graph\n",
        "  start_time = time.time()\n",
        "  tracemalloc.start()\n",
        "  G = nx.DiGraph()\n",
        "  primers_init = [Primer(p.start,p.stop) for _,p in primer_f.query('stop<=0').iterrows() if check_threshold(p.tm,p.gc)]  ## all forward primers that end before mutreg and pass threshold\n",
        "  for primer in primers_init:\n",
        "    G.add_edge('s',primer.tup(), weight=primer.w) #intializing the s-primer connection\n",
        "    dfs(primer) #create the rest of the graph\n",
        "\n",
        "  graph_time = int(time.time() - start_time)\n",
        "  graph_memory = tracemalloc.get_traced_memory()[1] / 10**6 #MB\n",
        "  tracemalloc.stop()\n",
        "\n",
        "  #Greedy Solution\n",
        "  start_time = time.time()\n",
        "  greedy_solution, greedy_obj = run_greedy(G, primer_f, primer_df)\n",
        "  greedy_time = time.time() - start_time\n",
        "\n",
        "  #Convert to ILP & Solve\n",
        "  tracemalloc.start()\n",
        "  start_time = time.time()\n",
        "\n",
        "  numVars, numConstrs, setup_time, setup_memory, ILP_time, ILP_memory, actual_values, objective = get_info([G, G])\n",
        "  #numVars, numConstrs, setup_time, setup_memory, ILP_time, ILP_memory = 0,0,0,0,0,0\n",
        "  all_data.append({\"seq length:\": seq_length,\n",
        "                    \"Nodes\":len(G.nodes),\n",
        "                    \"Edges\": len(G.edges),\n",
        "                    \"Time (Graph)\": graph_time,\n",
        "                    \"MP (Graph)\": graph_memory,\n",
        "                    \"Vars\": numVars,\n",
        "                    \"Constr\": numConstrs,\n",
        "                    \"Time (Setup)\": setup_time,\n",
        "                    \"MP (Setup)\": setup_memory,\n",
        "                    \"Time (ILP)\": ILP_time,\n",
        "                    \"MP (ILP)\": ILP_memory,\n",
        "                    \"ILP Solution\": actual_values,\n",
        "                    \"ILP Objective\": objective,\n",
        "                    \"Greedy Solution\": greedy_solution,\n",
        "                    \"Greedy Objective\": greedy_obj,\n",
        "                    \"Greedy Time\": greedy_time})\n",
        "  print(all_data[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrNTrVAMCXfX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming all_data is a list of dictionaries\n",
        "df = pd.DataFrame(all_data)\n",
        "\n",
        "# Write the DataFrame to a CSV file\n",
        "df.to_csv('data.csv', index=False)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "3-cQNE8To4yW",
        "MFTaJFH_qB2p",
        "WXfAgj6lqJQn",
        "3Ah8DwG1FeE6",
        "UzIt-1aCqUAH"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
