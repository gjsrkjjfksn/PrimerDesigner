{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OrensteinLab/PrimerDesigner/blob/main/Tracking_Timings_Creation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-cQNE8To4yW"
      },
      "source": [
        "#Pre-Calculations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFTaJFH_qB2p"
      },
      "source": [
        "##Installs & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5Qvu79FpBUL"
      },
      "outputs": [],
      "source": [
        "!pip install -U bokeh seaborn pandas\n",
        "!pip install primer3-py biopython pandarallel\n",
        "!pip install pulp\n",
        "!pip install gurobipy\n",
        "!pip install biopython==1.75\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nu59O-XqpDCW"
      },
      "outputs": [],
      "source": [
        "# IMPORTS\n",
        "import time\n",
        "\n",
        "import random as rand\n",
        "from itertools import product\n",
        "\n",
        "\n",
        "from pandarallel import pandarallel as pl\n",
        "pl.initialize()\n",
        "\n",
        "import primer3 as p3\n",
        "from Bio.Seq import Seq\n",
        "from Bio.SeqUtils import GC, seq1, seq3\n",
        "from Bio.SeqUtils.CodonUsage import SharpEcoliIndex, SynonymousCodons\n",
        "\n",
        "import itertools as it\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('display.precision', 1)\n",
        "import networkx as nx\n",
        "\n",
        "import gurobipy as gp\n",
        "\n",
        "import pickle\n",
        "\n",
        "import tracemalloc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def revcomp(seq):\n",
        "  return str(Seq(seq).reverse_complement())\n",
        "def translate(seq):\n",
        "  return str(Seq(seq).translate())\n",
        "\n",
        "# clear_output()\n",
        "print('Ready')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cj6-Rj3M55cT"
      },
      "source": [
        "## Find primer sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlY4e_TaqpUb"
      },
      "outputs": [],
      "source": [
        "def subsequences(sequence, lmin, lmax): #Generates all subsequences w/ all poss. start-stop pairs\n",
        "  ls = []\n",
        "  for j in range(lmin, lmax+1): #length\n",
        "    for i in range(len(sequence)-j+1): #starting index\n",
        "      start = i\n",
        "      stop = i+j\n",
        "      ls.append([sequence[start:stop], start, stop, stop-start])\n",
        "  return pd.DataFrame(ls, columns=['seq','start','stop','len'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VZtd11nmgBD"
      },
      "source": [
        "## Primer Data\n",
        "##### Creates the primer pandas dataframe containing information about every possible primer in the protein coding sequence. The function calculates various primer parameters including gc content, delta G and melting temepratures. The cost is also calculated for every primer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxLGrHMfqu60"
      },
      "outputs": [],
      "source": [
        "def create_primer_df(sequence_nt):\n",
        "  # convention: start index of r-primers will be 3' (i.e. start < stop)\n",
        "  primer_f = pd.DataFrame(columns=['seq','start','stop','fr','len'])\n",
        "  primer_f[['seq','start','stop','len']] = subsequences(sequence_nt, primer_lmin, primer_lmax)\n",
        "  primer_f['fr'] = 'f'\n",
        "\n",
        "  #Shifting so that 0 is at the start of mutreg (upstream has negative values)\n",
        "  primer_f['start'] = primer_f.start - len(upstream_nt)\n",
        "  primer_f['stop'] = primer_f.stop - len(upstream_nt)\n",
        "\n",
        "  #Creating reverse primers at same locations\n",
        "  primer_r = primer_f[['seq','start','stop','fr','len']].copy()\n",
        "  primer_r['fr'] = 'r'\n",
        "  primer_r['seq'] = primer_r.seq.apply(revcomp)\n",
        "\n",
        "  #Concatenating Forward & Reverse\n",
        "  primer_df = pd.concat([primer_f,primer_r])\n",
        "  primer_df.sort_values(by=['start','stop','fr'], inplace=True)\n",
        "\n",
        "  #Calculating \"Cost\" Values\n",
        "  primer_df['gc'] = primer_df.seq.apply(GC)\n",
        "  primer_df['tm'] = primer_df.seq.apply(pcr.calcTm)\n",
        "  res = primer_df.seq.parallel_apply(lambda s: pcr.calcHairpin(s).todict())\n",
        "  primer_df['hp_tm'] = res.apply(lambda res: res['tm'])\n",
        "  primer_df['hp_dg'] = res.apply(lambda res: res['dg']*1e-3)\n",
        "  res = primer_df.seq.parallel_apply(lambda s: pcr.calcHomodimer(s).todict())\n",
        "  primer_df['ho_tm'] = res.apply(lambda res: res['tm'])\n",
        "  primer_df['ho_dg'] = res.apply(lambda res: res['dg']*1e-3)\n",
        "\n",
        "  def primer_cost(primer):\n",
        "    # calculates primer cost based on homodimer an haipin delta G, tm cost, and len cost\n",
        "    hp_dg_max = -4\n",
        "    ho_dg_max = -8\n",
        "    tm_min = 58\n",
        "\n",
        "    tm_cost = max(0, tm_min-primer.tm)**1.7\n",
        "    hp_cost = max(0, hp_dg_max - primer.hp_dg)**1.2\n",
        "    ho_cost = max(0, ho_dg_max - primer.ho_dg)**1.2\n",
        "    len_cost = primer.len*1e-5\n",
        "\n",
        "    cost = hp_cost + ho_cost + len_cost + tm_cost\n",
        "\n",
        "    sequence = primer.seq\n",
        "    for nucleotide in \"ATGC\":\n",
        "        if nucleotide * 4 in sequence:\n",
        "            cost += 1000000\n",
        "            break\n",
        "\n",
        "    return cost\n",
        "\n",
        "  primer_df['cost'] = primer_df.parallel_apply(primer_cost, axis=1)\n",
        "  primer_df['log10cost'] = primer_df.cost.apply(np.log10)\n",
        "\n",
        "  primer_df.reset_index(inplace=True)\n",
        "  primer_f = primer_df.query('fr==\"f\"').reset_index(drop=True)\n",
        "  primer_r = primer_df.query('fr==\"r\"').reset_index(drop=True)\n",
        "  primer_df.set_index(['start','stop','fr'], inplace=True)\n",
        "\n",
        "  return primer_f, primer_df\n",
        "\n",
        "def check_threshold(tm,gc):\n",
        "  # returns false only if the threshold flag is on and primers did not pass threshold\n",
        "  if not apply_threshold:\n",
        "    return True\n",
        "  else:\n",
        "    if min_gc <= gc <= max_gc and  min_tm <= tm <=max_tm:\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAbrEHcQnqXS"
      },
      "source": [
        "## Primer Class\n",
        "##### Creates a primer class which is used for the creation of the primer graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9Vws9ri2rd2"
      },
      "outputs": [],
      "source": [
        "class Primer:\n",
        "  def __init__(self,primer_df,start,stop,is_r=False):\n",
        "    assert start < stop\n",
        "    self.start = start\n",
        "    self.stop = stop\n",
        "    self.is_r = is_r #forward or reverse\n",
        "    self.l = stop-start #length\n",
        "    self.primer_df = primer_df\n",
        "    self.w = primer_df.at[self.tup(),'cost'] #total cost value; the fancy notation is b/c\n",
        "                                             #of the hierarchal lookup system in panda.df\n",
        "\n",
        "  def __str__(self):\n",
        "    return ' '.join(map(str,(self.start, self.stop, self.is_r)))\n",
        "  def __repr__(self):\n",
        "    return f'{(\"r\" if self.is_r else\"f\")}({self.start},{self.stop})'\n",
        "  def tup(self):\n",
        "    return (self.start,self.stop,(\"r\" if self.is_r else\"f\"))\n",
        "\n",
        "\n",
        "def actions(primer_df,primer): #returning possible counterparts (forward -> reverse; reverse -> forward)\n",
        "                    #i.e. this method gets the \"neighbors\"\n",
        "  if not primer.is_r:  # fwd\n",
        "    for oligo_l, primer_l in it.product(reversed(range(oligo_lmin, oligo_lmax+1)),\n",
        "                                        range(primer_lmin,primer_lmax+1)):\n",
        "\n",
        "      stop = primer.start + oligo_l\n",
        "      start = stop - primer_l\n",
        "\n",
        "      if apply_threshold:\n",
        "        # finds tm of forward and reverse primers\n",
        "        tm_f=primer_df.at[primer.tup(),'tm']\n",
        "        tm_r=primer_df.at[(start,stop,\"r\"),'tm']\n",
        "\n",
        "        # if tm difference is larger then max_difference threshold do not add primer to graph\n",
        "        if abs(tm_f-tm_r)>max_difference:\n",
        "          continue\n",
        "\n",
        "      yield Primer(primer_df,start, stop, is_r=True)\n",
        "\n",
        "  elif primer.is_r:  # rev\n",
        "    for overlap_l, primer_l in it.product(reversed(range(overlap_lmin, overlap_lmax+1)),\n",
        "                                          range(primer_lmin,primer_lmax+1)):\n",
        "\n",
        "      start = primer.stop - overlap_l\n",
        "      stop = start + primer_l\n",
        "\n",
        "\n",
        "      # filter\n",
        "      no_split = (primer.start - stop) >= primer.start%3\n",
        "      if (stop > primer.start) or (not no_split): ## redundant to check first condition?\n",
        "        continue\n",
        "      yield Primer(primer_df,start, stop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP2ck_c5nkiS"
      },
      "source": [
        "## Graph DFS\n",
        "##### Recursive function used for creating the primer graph representing all possible forward and reverse primer combinations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gal-RcyG2uBH"
      },
      "outputs": [],
      "source": [
        "def dfs(G, primer, primer_df, mutreg_l): #CREATING the graph\n",
        "\n",
        "  tm=primer_df.at[primer.tup(),'tm']\n",
        "  gc=primer_df.at[primer.tup(),'gc']\n",
        "\n",
        "  if (primer.start >= mutreg_l) and primer.is_r and check_threshold(tm,gc) :  # base case (end)\n",
        "    G.add_edge(primer.tup(),'d', weight=0.) #G is global variable defined in next section\n",
        "    return\n",
        "\n",
        "  for next_primer in actions(primer_df,primer):\n",
        "\n",
        "    is_new = not G.has_node(next_primer.tup()) # check if primer node exists already\n",
        "\n",
        "    tm=primer_df.at[next_primer.tup(),'tm']\n",
        "    gc=primer_df.at[next_primer.tup(),'gc']\n",
        "\n",
        "    passes_threshold= check_threshold(tm,gc) # check if primer passes threshold\n",
        "\n",
        "    # only add edge if primer passes threshold\n",
        "    if passes_threshold:\n",
        "      G.add_edge(primer.tup(),next_primer.tup(), weight=next_primer.w) #weight is the cost of the new primer\n",
        "    if passes_threshold and is_new:\n",
        "      dfs(G,next_primer,primer_df,mutreg_l)\n",
        "\n",
        "def paths_ct(G, u, d): #total # of paths between two points\n",
        "    if u == d:\n",
        "        return 1\n",
        "    else:\n",
        "        if not G.nodes[u]: #npaths attribute is the # of paths out of u\n",
        "            G.nodes[u]['npaths'] = sum(paths_ct(G, c, d) for c in G.successors(u))\n",
        "        return G.nodes[u]['npaths']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DXHCtTQnVmR"
      },
      "source": [
        "## Greedy Algorithms\n",
        "##### Runs the baseline greedy algorithm which is compared to PrimerDesigner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceiNxaB-D1z1"
      },
      "outputs": [],
      "source": [
        "def run_greedy(graphs, primer_dfs,multiple_forbidden,protein_names):\n",
        "\n",
        "  path_ls = []\n",
        "\n",
        "  for protein in protein_names:\n",
        "    nodes_to_remove = []\n",
        "    G = graphs[protein]\n",
        "    G_sub = G.copy()\n",
        "    for i,path in enumerate(path_ls):\n",
        "      other_protein = protein_names[i]\n",
        "      for p,n in it.product(path,G_sub.nodes()):\n",
        "        if n=='s' or n=='d':\n",
        "          continue\n",
        "        pn_forbidden = ((p[0],p[1]),(n[0],n[1])) in multiple_forbidden[(other_protein,protein)]\n",
        "        if pn_forbidden:\n",
        "          nodes_to_remove.append(n)\n",
        "    G_sub.remove_nodes_from(set(nodes_to_remove))\n",
        "    try:\n",
        "      path_ls.append([primer for primer in nx.algorithms.shortest_path(G_sub,'s','d', weight='weight')][1:-1])\n",
        "    except:\n",
        "      print(f'WARNING: No feasible primer sequence for lib_{i}; reduce number of libraries or relax constraints.')\n",
        "\n",
        "  # # https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html\n",
        "  primer_set = pd.DataFrame()\n",
        "  for i,primer_ls in enumerate(path_ls):\n",
        "    protein = protein_names[i]\n",
        "    primer_df = primer_dfs[protein]\n",
        "    primer_set1 = primer_df.loc[primer_ls].copy().reset_index()\n",
        "    primer_set1['lib_i'] = protein\n",
        "    primer_set = pd.concat([primer_set, primer_set1])\n",
        "  primer_set['tile_i'] = primer_set.groupby(['lib_i','fr']).cumcount()\n",
        "  primer_set = primer_set[['lib_i','tile_i']+primer_set.columns[:-2].to_list()]\n",
        "\n",
        "  #print(primer_set.groupby('lib_i').cost.sum())\n",
        "  cost = primer_set.cost.sum()\n",
        "\n",
        "  return path_ls, cost\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ah8DwG1FeE6"
      },
      "source": [
        "##ILP Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZB76G7WmE9m"
      },
      "source": [
        "## Gurobi Setup\n",
        "##### The user should input their Gurobi license details for the ILP part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unOF64OiDddN"
      },
      "outputs": [],
      "source": [
        "def get_model():\n",
        "  params = {\n",
        "  \"WLSACCESSID\":\"\",\n",
        "  \"WLSSECRET\":\"\",\n",
        "  \"LICENSEID\":\n",
        "  }\n",
        "  env = gp.Env(params=params)\n",
        "\n",
        "  # Create the model within the Gurobi environment\n",
        "  model = gp.Model('min-sum', env=env)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AS8AHMSPoio-"
      },
      "source": [
        "## Single protein constraints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "er2HkwrVn6sN"
      },
      "outputs": [],
      "source": [
        "\n",
        "def single_pairs(protein_names,sequences_nt):\n",
        "\n",
        "  single_forbidden={}\n",
        "  for name,sequence in zip(protein_names,sequences_nt):\n",
        "    # find forbidden_pairs in single protein\n",
        "    pairs_finder = PairFinder(sequence)\n",
        "    forbidden_pairs = pairs_finder.find_all_pairs()\n",
        "    single_forbidden[name]=forbidden_pairs\n",
        "\n",
        "  return single_forbidden\n",
        "\n",
        "def single_constraints(graph,forbidden_pairs,model):\n",
        "\n",
        "    #Getting Nodes/Edges\n",
        "    graph_edges = graph.edges(data=True)\n",
        "    graph_nodes = [node for node in graph.nodes if node != 's' and node != 'd'] #removing s & d nodes\n",
        "\n",
        "    #Converting Graphs to Lists of model variables\n",
        "    ij = gp.tuplelist()\n",
        "    w_ij = gp.tupledict()\n",
        "\n",
        "    for edge in graph_edges:\n",
        "      l = (str(edge[0]), str(edge[1])) #i, j\n",
        "      ij.append(l)\n",
        "      w_ij[l] = edge[-1]['weight']\n",
        "    print(\"Finished Conversion\")\n",
        "\n",
        "    # Adding Variables to model\n",
        "    protein_vars = model.addVars(ij, obj=w_ij, vtype=gp.GRB.BINARY)\n",
        "    print(\"Finished Variable Creations\")\n",
        "\n",
        "    #Single Path Constraints\n",
        "    for n in graph_nodes + ['s', 'd']: #adding s & d back just here\n",
        "      v = str(n)\n",
        "      model.addConstr(sum(protein_vars[i,j] for i,j in ij.select(v, '*')) - sum(protein_vars[j,i] for j,i in ij.select('*', v)) == (1 if v=='s' else -1 if v=='d' else 0), v)\n",
        "\n",
        "    cnt=0\n",
        "    # Intersection Constraints - forbidden primer pairs\n",
        "    for cnt,pairs in enumerate(forbidden_pairs):\n",
        "      all_edges = []\n",
        "      for pair in pairs:\n",
        "        node1 = (pair[0],pair[1],'f')\n",
        "        node2 = (pair[0],pair[1],'r')\n",
        "        all_edges.append(protein_vars.sum(str(node1), '*'))\n",
        "        all_edges.append(protein_vars.sum(str(node2), '*'))\n",
        "      model.addConstr(gp.quicksum(all_edges) <= 1)\n",
        "\n",
        "    print(\"Finished single forbiden pairs constraints!\")\n",
        "    print(\"Number of single forbidden pairs: \",cnt)\n",
        "\n",
        "    return protein_vars\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YB23fvvqkNr"
      },
      "source": [
        "## Add multiple protein constraints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sqC7GXWqjAI"
      },
      "outputs": [],
      "source": [
        "from itertools import combinations\n",
        "\n",
        "def multiple_pairs(protein_names,sequences_nt):\n",
        "\n",
        "    multiple_forbidden={}\n",
        "\n",
        "    protein_pairs = list(combinations(range(len(protein_names)), 2))\n",
        "\n",
        "    for p1,p2 in protein_pairs:\n",
        "      sequence1=sequences_nt[p1]\n",
        "      sequence2=sequences_nt[p2]\n",
        "\n",
        "      protein1=protein_names[p1]\n",
        "      protein2=protein_names[p2]\n",
        "\n",
        "      pairs_finder = PairFinder(sequence1, sequence2)\n",
        "      forbidden_pairs = pairs_finder.find_all_pairs()\n",
        "\n",
        "      multiple_forbidden[(protein1,protein2)]=forbidden_pairs\n",
        "\n",
        "    return multiple_forbidden\n",
        "\n",
        "\n",
        "def multiple_constraints(model, model_vars, multiple_forbidden):\n",
        "\n",
        "    total_forbidden_pairs = 0\n",
        "\n",
        "    for proteins, forbidden_pairs in multiple_forbidden.items():\n",
        "\n",
        "        protein1, protein2 = proteins\n",
        "\n",
        "        x = model_vars[protein1]\n",
        "        y = model_vars[protein2]\n",
        "\n",
        "        # add <= constraint for every forbidden pair\n",
        "        for forbidden_pair in forbidden_pairs:\n",
        "\n",
        "            all_edges = []\n",
        "            pair1, pair2 = forbidden_pair\n",
        "\n",
        "            node_x1 = (pair1[0], pair1[1], 'f')\n",
        "            node_x2 = (pair1[0], pair1[1], 'r')\n",
        "            all_edges.append(x.sum(str(node_x1), '*'))\n",
        "            all_edges.append(x.sum(str(node_x2), '*'))\n",
        "\n",
        "            node_y1 = (pair2[0], pair2[1], 'f')\n",
        "            node_y2 = (pair2[0], pair2[1], 'r')\n",
        "            all_edges.append(y.sum(str(node_y1), '*'))\n",
        "            all_edges.append(y.sum(str(node_y2), '*'))\n",
        "\n",
        "            # constraint: the sum of outgoing edges of fobidden pairs <=1\n",
        "            model.addConstr(gp.quicksum(all_edges) <= 1)\n",
        "            total_forbidden_pairs += 1\n",
        "\n",
        "\n",
        "    # Print the total number of constraints added after processing all combinations\n",
        "    print(\"Finished adding multiple forbidden pairs constraints.\")\n",
        "    print(\"Total number of forbidden pairs constraints added:\", total_forbidden_pairs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vMxJ-xwaCcN"
      },
      "source": [
        "## Forbidden pairs finder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8vZr0QKaBYV"
      },
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "primer_lmin, primer_lmax = 18, 30\n",
        "overlap = primer_lmax\n",
        "\n",
        "class PairFinder:\n",
        "\n",
        "  def __init__(self, sequence_nt1, sequence_nt2=None):\n",
        "        self.sequence_nt1 = sequence_nt1\n",
        "        self.sequence_nt2 = sequence_nt2 if sequence_nt2 else sequence_nt1\n",
        "\n",
        "  def forbidden_pairs(self,primer, p_end, sequence_nt):\n",
        "\n",
        "    memo={}\n",
        "\n",
        "    def search(start, end):\n",
        "        results = []\n",
        "\n",
        "        # Base case\n",
        "        if (end-start) <= 2*(overlap):\n",
        "\n",
        "          tm = p3.bindings.calc_heterodimer(sequence_nt[start:end], primer, mv_conc=50.0, dv_conc=1.5, dntp_conc=0.6,\n",
        "                                                  dna_conc=50.0, temp_c=37.0, max_loop=30).tm\n",
        "          if tm>=max_tm:\n",
        "            return handle_special_case(start, end)\n",
        "          else:\n",
        "            return []\n",
        "\n",
        "        mid = (start + end) // 2\n",
        "        left_end = mid + (overlap-1)\n",
        "        right_start = max(mid - (overlap-1),0)\n",
        "\n",
        "        first_tm = p3.bindings.calc_heterodimer(sequence_nt[start:left_end], primer, mv_conc=50.0, dv_conc=1.5, dntp_conc=0.6,\n",
        "                                                  dna_conc=50.0, temp_c=37.0, max_loop=30).tm\n",
        "\n",
        "        second_tm = p3.bindings.calc_heterodimer(sequence_nt[right_start:end], primer, mv_conc=50.0, dv_conc=1.5, dntp_conc=0.6,\n",
        "                                            dna_conc=50.0, temp_c=37.0, max_loop=30).tm\n",
        "\n",
        "        # Check and recurse on the left segment\n",
        "        if first_tm >= max_tm:\n",
        "            results.extend(search(start, left_end))\n",
        "\n",
        "        # Check and recurse on the right segment\n",
        "        if second_tm >= max_tm:\n",
        "            results.extend(search(right_start, end))\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "    def handle_special_case(start, end):\n",
        "        if (start, end) in memo:\n",
        "            return memo[(start, end)]\n",
        "\n",
        "        # Avoid unnecessary recursion if the segment is already at the minimum length\n",
        "        if end - start <= primer_lmax:\n",
        "            tm =  p3.bindings.calc_heterodimer(sequence_nt[start:end], primer, mv_conc=50.0, dv_conc=1.5, dntp_conc=0.6,\n",
        "                                                  dna_conc=50.0, temp_c=37.0, max_loop=30).tm\n",
        "            if tm >= max_tm:\n",
        "                memo[(start, end)] = [(start, end)]\n",
        "                return [(start, end)]\n",
        "            else:\n",
        "                memo[(start, end)] = []\n",
        "                return []\n",
        "\n",
        "        # Recursive case: try shortening the segment from both ends\n",
        "        shorten_left = handle_special_case(start + 1, end)\n",
        "        shorten_right = handle_special_case(start, end - 1)\n",
        "\n",
        "        result = list(set(shorten_left+shorten_right))\n",
        "\n",
        "        memo[(start, end)] = result\n",
        "        return result\n",
        "\n",
        "    off_targets=[]\n",
        "\n",
        "    # search second half of sequence (with max allowed overlap)\n",
        "    off_targets += search(max(p_end-allowed_overlap,0),len(sequence_nt))\n",
        "\n",
        "    return off_targets\n",
        "\n",
        "  def find_all_pairs(self):\n",
        "\n",
        "    forbidden_pairs = []\n",
        "    # iterate over every start position\n",
        "    for p_start in range(len(self.sequence_nt1)-primer_lmax+1):\n",
        "        p_end=p_start + primer_lmax\n",
        "        primer = self.sequence_nt1[p_start:p_end]\n",
        "        # single sequence logic\n",
        "        if self.sequence_nt1 == self.sequence_nt2:\n",
        "          off_targets = self.forbidden_pairs(primer,p_end,self.sequence_nt1)\n",
        "        else: # multiple sequence logic\n",
        "          off_targets = self.forbidden_pairs(primer,0,self.sequence_nt2)\n",
        "\n",
        "        for target in off_targets:\n",
        "            forbidden_pairs+=[((p_start,p_end),target)]\n",
        "\n",
        "    forbidden_pairs=set(forbidden_pairs)\n",
        "\n",
        "    sub_pairs = self.sub_pairs_parallel(forbidden_pairs)\n",
        "\n",
        "    return sub_pairs\n",
        "\n",
        "  def process_pair(self,pair):\n",
        "    sub_forbidden_pairs = []\n",
        "    p1, p2 = pair\n",
        "\n",
        "    primer1 = self.sequence_nt1[p1[0]:p1[1]]\n",
        "    primer2 = self.sequence_nt2[p2[0]:p2[1]]\n",
        "\n",
        "    for length1 in range(primer_lmin, primer_lmax+1):\n",
        "        for length2 in range(primer_lmin, primer_lmax+1):\n",
        "            for start1 in range(primer_lmax - length1 + 1):\n",
        "                for start2 in range(primer_lmax - length2 + 1):\n",
        "                    sub1 = primer1[start1:start1 + length1]\n",
        "                    sub2 = primer2[start2:start2 + length2]\n",
        "                    tm = p3.bindings.calc_heterodimer(sub1, sub2, mv_conc=50.0, dv_conc=1.5, dntp_conc=0.6,\n",
        "                                                      dna_conc=50.0, temp_c=37.0, max_loop=30).tm\n",
        "                    if tm >= max_tm:\n",
        "                        sub_forbidden_pairs.append(((start1+p1[0]-mutreg_start, start1 + p1[0] + length1-mutreg_start), (start2 + p2[0]-mutreg_start, start2 + p2[0] + length2 - mutreg_start)))\n",
        "\n",
        "    return sub_forbidden_pairs\n",
        "\n",
        "  def sub_pairs_parallel(self,forbidden_pairs):\n",
        "\n",
        "      pool = multiprocessing.Pool(processes=multiprocessing.cpu_count())\n",
        "      result = pool.map(self.process_pair, forbidden_pairs)\n",
        "      pool.close()\n",
        "      pool.join()\n",
        "\n",
        "      # Flatten the list of lists\n",
        "      sub_forbidden_pairs = set([item for sublist in result for item in sublist])\n",
        "\n",
        "      return sub_forbidden_pairs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzIt-1aCqUAH"
      },
      "source": [
        "##Full Sequences\n",
        "##### The user can choose the mutreg sequence and upstream and downstream regions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfCGd8GAqMDQ"
      },
      "outputs": [],
      "source": [
        "# constant upstream and downstream regions for 10 proteins\n",
        "upstream_nt = 'GCTAGTGGTGCTAGCCCCGCGAAATTAATACGACTCTCTATAGGGTCTAGAAATAATTTTCTTTAACTTTAAGAAGGAGATATACAT'\n",
        "downstream_nt = 'GGAGGGTCTGGGGCAGGAGGCAGTGGCATGGTGACCAAGGGCGTGGAGCTGTTCACCGGGGTGGTGCCCATCCTGGTCGAGCTGGACGGCGACGTAAACGGCCACAAGTTCAGCGTGTCCGGCGAGGGCGAGGGCGATGCCACCTACGGCAAGCTGACCCTGAAGTTCATCTGCACCACCGGCAAGCTGCCCG'\n",
        "\n",
        "mutreg_regions=[]\n",
        "protein_names=[]\n",
        "\n",
        "# read 10 protein coding sequences from file\n",
        "with open('10 coding sequences.txt') as file:\n",
        "  for line in file.readlines():\n",
        "    p_name, mutreg_region = line.strip().split('\\t')\n",
        "    mutreg_regions.append(mutreg_region)\n",
        "    protein_names.append(p_name)\n",
        "\n",
        "sequences_nt=[]\n",
        "\n",
        "for mutreg_nt in mutreg_regions:\n",
        "  sequence= upstream_nt + mutreg_nt + downstream_nt\n",
        "  sequences_nt.append(sequence)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1juYXyJqiRq"
      },
      "source": [
        "##Parameters\n",
        "##### The user can adjust algorithm parameters such as primer length, oligo length and allowed overlap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BthzfpIVklkP"
      },
      "outputs": [],
      "source": [
        "# adjust algorithm parameters\n",
        "primer_lmin, primer_lmax = 18, 30\n",
        "overlap_lmin,overlap_lmax = 45,50\n",
        "oligo_lmin,oligo_lmax = 195,205\n",
        "allowed_overlap = 6\n",
        "mutreg_start = len(upstream_nt)\n",
        "\n",
        "# sets up pcr reaction with parameters\n",
        "pcr = p3.thermoanalysis.ThermoAnalysis(dna_conc= 250,\n",
        "                                       mv_conc= 50,\n",
        "                                       dv_conc= 0,\n",
        "                                       dntp_conc= 0,\n",
        "                                       tm_method= 'santalucia',\n",
        "                                       salt_correction_method= 'owczarzy',\n",
        "                                       temp_c= 25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0URNP9iplG5S"
      },
      "source": [
        "## Efficiency Thresholds\n",
        "##### The user can choose to apply primer efficiency threshold on gc content and tm and the maximum allowed tm different between forward and reverse primer in each pair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5Ia232clNyo"
      },
      "outputs": [],
      "source": [
        "apply_threshold= False # apply threshold flag. If true, threshold will be applied\n",
        "min_gc=40\n",
        "max_gc=60\n",
        "min_tm=58\n",
        "max_tm=65\n",
        "max_difference=3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDsUII2TqmbD"
      },
      "source": [
        "##Create primer graphs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7M-HW4-5bfV"
      },
      "outputs": [],
      "source": [
        "def create_graphs(mutreg_regions,sequences_nt,protein_names):\n",
        "\n",
        "  #Creating the Graph\n",
        "  start_time = time.time()\n",
        "  tracemalloc.start()\n",
        "\n",
        "  graphs={}\n",
        "  primer_dfs={}\n",
        "\n",
        "  for mutreg_nt,sequence,protein_name in zip(mutreg_regions,sequences_nt,protein_names):\n",
        "\n",
        "    mutreg_l = len(mutreg_nt)\n",
        "\n",
        "    primer_f, primer_df = create_primer_df(sequence)\n",
        "\n",
        "    primer_dfs[protein_name] = primer_df\n",
        "\n",
        "    G = nx.DiGraph()\n",
        "    primers_init = [Primer(primer_df,p.start,p.stop) for _,p in primer_f.query('stop<=0').iterrows() if check_threshold(p.tm,p.gc)]  ## all forward primers that end before mutreg and pass threshold\n",
        "    for primer in primers_init:\n",
        "      G.add_edge('s',primer.tup(), weight=primer.w) #intializing the s-primer connection\n",
        "      dfs(G, primer,primer_df,mutreg_l) #create the rest of the graph\n",
        "\n",
        "    graphs[protein_name]=G\n",
        "\n",
        "    print(\"Nodes: \", len(G.nodes))\n",
        "\n",
        "  graph_time = int(time.time() - start_time)\n",
        "  graph_memory = tracemalloc.get_traced_memory()[1] / 10**6 #MB\n",
        "  tracemalloc.stop()\n",
        "\n",
        "  all_nodes=0\n",
        "  all_edges=0\n",
        "\n",
        "  for G in graphs.values():\n",
        "    all_nodes += len(G.nodes)\n",
        "    all_edges += len(G.edges)\n",
        "\n",
        "  return graphs, graph_time, graph_memory, all_nodes,all_edges, primer_dfs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13Zl-7YWXwFA"
      },
      "source": [
        "## Model Creation and Run\n",
        "\n",
        "##### This function creates the ILP solver and represents the primer graph as and ILP optimization problem. It adds the ILP contraints based on overlap and single path contraints. Then, it runs the ILP solver and returns primer path solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OfGAGHVXiR6"
      },
      "outputs": [],
      "source": [
        "\n",
        "def run_ilp(single_forbidden,multiple_forbidden,protein_names,graphs):\n",
        "\n",
        "  # measure setup time\n",
        "  setup_start = time.time()\n",
        "  tracemalloc.start()\n",
        "  #Create ILP model\n",
        "  model = get_model()\n",
        "\n",
        "  # list of all groups of variables for each protein\n",
        "  protein_vars={}\n",
        "\n",
        "  ## call function to add single path and single forbidden pair ILP constraints\n",
        "  for name in protein_names:\n",
        "    graph = graphs[name]\n",
        "    forbidden_pairs=single_forbidden[name]\n",
        "    vars = single_constraints(graph,forbidden_pairs,model) # returns group of variables for protein\n",
        "    protein_vars[name]=vars\n",
        "\n",
        "  ## call function to add forbidden pair constraint between each protein pair\n",
        "  multiple_constraints(model, protein_vars, multiple_forbidden)\n",
        "\n",
        "  setup_time = time.time() - setup_start\n",
        "  setup_memory = tracemalloc.get_traced_memory()[1] / 10**6\n",
        "  tracemalloc.stop()\n",
        "\n",
        "  tracemalloc.start()\n",
        "  start_time = time.time()\n",
        "  model.optimize()\n",
        "  ILP_time = time.time() - start_time\n",
        "  ILP_memory = tracemalloc.get_traced_memory()[1] / 10**6\n",
        "  tracemalloc.stop()\n",
        "\n",
        "  def post_processing(x):\n",
        "    path = ['s']\n",
        "    true_edges = [index for index, var in x.items() if var.X != 0]\n",
        "\n",
        "    while true_edges:\n",
        "      edge = true_edges.pop(0)\n",
        "      added_edge = False\n",
        "      if edge[0] == path[-1]:\n",
        "        path.append(edge[1])\n",
        "        added_edge = True\n",
        "      if not added_edge:\n",
        "        true_edges.append(edge)\n",
        "\n",
        "    return path\n",
        "\n",
        "  protein_paths={}\n",
        "\n",
        "  for name,x in protein_vars.items():\n",
        "    actual_values = post_processing(x)\n",
        "    protein_paths[name]=actual_values\n",
        "  for name,vals in protein_paths.items():\n",
        "    print(f\"Protein #{name} ({len(vals)})\")\n",
        "    print(vals)\n",
        "    print()\n",
        "\n",
        "  return model.numVars, model.numConstrs, setup_time, setup_memory, ILP_time, ILP_memory, protein_paths, model.objVal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yT0AA0lH5kzT"
      },
      "source": [
        "## Run ILP and greedy algorithm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzokpLDXX59g"
      },
      "outputs": [],
      "source": [
        "all_data = []\n",
        "max_tm=45\n",
        "\n",
        "# iterate over all protein numbers from 2 to 10\n",
        "for i in range(2,11):\n",
        "\n",
        "  ## create graphs\n",
        "  graphs, graph_time, graph_memory,all_nodes, all_edges, primer_dfs = create_graphs(mutreg_regions[:i],sequences_nt[:i],protein_names[:i])\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  # call function create dictionary of forbidden primer pairs per single protein\n",
        "  single_forbidden = single_pairs(protein_names[:i],sequences_nt[:i])\n",
        "\n",
        "  single_pairs_time=time.time()-start_time\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  # call function that returns forbidden protein pair between multiple proteins\n",
        "  multiple_forbidden = multiple_pairs(protein_names[:i],sequences_nt[:i])\n",
        "\n",
        "  multi_pairs_time=time.time()-start_time\n",
        "\n",
        "  ## Greedy Solution\n",
        "  start_time = time.time()\n",
        "  greedy_solution, greedy_obj = run_greedy(graphs,primer_dfs,multiple_forbidden,protein_names[:i])\n",
        "  greedy_time = time.time() - start_time\n",
        "\n",
        "  # call function to run ILP\n",
        "  numVars, numConstrs, setup_time, setup_memory, ILP_time, ILP_memory, protein_paths, objective = run_ilp(single_forbidden,multiple_forbidden,protein_names[:i],graphs)\n",
        "\n",
        "\n",
        "  all_data.append({\"num proteins:\": i,\n",
        "                    \"Nodes\":all_nodes,\n",
        "                    \"Edges\": all_edges,\n",
        "                    \"Time (Graph)\": graph_time,\n",
        "                    \"MP (Graph)\": graph_memory,\n",
        "                    \"Vars\": numVars,\n",
        "                    \"Constr\": numConstrs,\n",
        "                    \"Time (Setup)\": setup_time,\n",
        "                    \"MP (Setup)\": setup_memory,\n",
        "                    \"multi pair time\": multi_pairs_time,\n",
        "                    \"single pair time\": single_pairs_time,\n",
        "                    \"Time (ILP)\": ILP_time,\n",
        "                    \"MP (ILP)\": ILP_memory,\n",
        "                    \"ILP Solution\": protein_paths,\n",
        "                    \"ILP Objective\": objective,\n",
        "                    \"Greedy Solution\": greedy_solution,\n",
        "                    \"Greedy Objective\": greedy_obj,\n",
        "                    \"Greedy Time\": greedy_time})\n",
        "\n",
        "  print(all_data[-1])\n",
        "\n",
        "  run_df = pd.DataFrame(all_data)\n",
        "\n",
        "  # Write the DataFrame to a CSV file\n",
        "  run_df.to_csv('run_data.csv', index=False)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "3-cQNE8To4yW",
        "MFTaJFH_qB2p",
        "WXfAgj6lqJQn",
        "3Ah8DwG1FeE6",
        "UzIt-1aCqUAH"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
